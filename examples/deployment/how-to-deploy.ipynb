{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config(path=\".file-path/ws_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model onnx_mnist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Download model\n",
    "onnx_model_url = \"https://www.cntk.ai/OnnxModels/mnist/opset_7/mnist.tar.gz\"\n",
    "urllib.request.urlretrieve(onnx_model_url, filename=\"mnist.tar.gz\")\n",
    "os.system('tar xvzf mnist.tar.gz')\n",
    "# Register model\n",
    "model = Model.register(workspace = ws,\n",
    "                        model_path =\"mnist/model.onnx\",\n",
    "                        model_name = \"onnx_mnist\",\n",
    "                        tags = {\"onnx\": \"demo\"},\n",
    "                        description = \"MNIST image classification CNN from ONNX Model Zoo\",)\n",
    "\n",
    "#model = Model(ws,'onnx_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a driver file\n",
    "To run a model in Azure ML, you need to define a driver file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\n",
      "import numpy as np\n",
      "import onnxruntime\n",
      "import sys\n",
      "import os\n",
      "from azureml.core.model import Model\n",
      "import time\n",
      "\n",
      "\n",
      "def init():\n",
      "    global session, input_name, output_name\n",
      "    model = os.path.join(os.getenv('AZUREML_MODEL_DIR'),'model.onnx')\n",
      "    session = onnxruntime.InferenceSession(model, None)\n",
      "    input_name = session.get_inputs()[0].name\n",
      "    output_name = session.get_outputs()[0].name \n",
      "    \n",
      "\n",
      "def preprocess(input_data_json):\n",
      "    # convert the JSON data into the tensor input\n",
      "    return np.array(json.loads(input_data_json)['data']).astype('float32')\n",
      "\n",
      "def postprocess(result):\n",
      "    # We use argmax to pick the highest confidence label\n",
      "    return int(np.argmax(np.array(result).squeeze(), axis=0))\n",
      "    \n",
      "def run(input_data):\n",
      "\n",
      "    try:\n",
      "        # load in our data, convert to readable format\n",
      "        data = preprocess(input_data)\n",
      "        \n",
      "        # start timer\n",
      "        start = time.time()\n",
      "        \n",
      "        r = session.run([output_name], {input_name: data})\n",
      "        \n",
      "        #end timer\n",
      "        end = time.time()\n",
      "        \n",
      "        result = postprocess(r)\n",
      "        result_dict = {\"result\": result,\n",
      "                      \"time_in_sec\": end - start}\n",
      "    except Exception as e:\n",
      "        result_dict = {\"error\": str(e)}\n",
      "    \n",
      "    return result_dict\n",
      "\n",
      "def choose_class(result_prob):\n",
      "    \"\"\"We use argmax to determine the right label to choose from our output\"\"\"\n",
      "    return int(np.argmax(result_prob, axis=0))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"onnx/onnx_score.py\", \"r\") as driver_file:\n",
    "    print(driver_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create inference environment\n",
    "Inference environments can be specified from pip / conda / a dockerfile.\n",
    "\n",
    "This example uses conda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment(Name: onnx_mnist,\n",
      "Version: None)\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "import os\n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=[\"numpy\", \"onnxruntime\", \"azureml-defaults\"])\n",
    "\n",
    "with open(os.path.join(\"onnx\",\"onnx_env.yml\"),\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "    \n",
    "deploy_env = Environment.from_conda_specification(\"onnx_mnist\",\"onnx/onnx_env.yml\")\n",
    "print(deploy_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define InferenceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(environment=deploy_env, \n",
    "                                   entry_script=\"onnx/onnx_score.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "from azureml.core import Model\n",
    "\n",
    "local_deploy_config = LocalWebservice.deploy_configuration(port=6789)\n",
    "local_service = Model.deploy(ws, \"testlocal\", [model], inference_config, local_deploy_config)\n",
    "local_service.wait_for_deployment()\n",
    "print('Local service port: {}'.format(local_service.port))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service onnx-demo-mnist-aci\n",
      "Running..............................\n",
      "SucceededACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {'demo': 'onnx'}, \n",
    "                                               description = 'ONNX for mnist model')\n",
    "aci_service_name = 'onnx-demo-mnist-aci'\n",
    "print(\"Service\", aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging your service\n",
    "Deployed services can be debugged by running the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/bin/bash: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libtinfo.so.5: no version information available (required by /bin/bash)\\n/bin/bash: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libtinfo.so.5: no version information available (required by /bin/bash)\\n/bin/bash: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libtinfo.so.5: no version information available (required by /bin/bash)\\n/bin/bash: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libtinfo.so.5: no version information available (required by /bin/bash)\\n2019-10-01T16:26:28,019934003+00:00 - gunicorn/run \\n2019-10-01T16:26:28,017860485+00:00 - iot-server/run \\n2019-10-01T16:26:28,021331715+00:00 - rsyslog/run \\nbash: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libtinfo.so.5: no version information available (required by bash)\\n2019-10-01T16:26:28,026211858+00:00 - nginx/run \\n/usr/sbin/nginx: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\\n/usr/sbin/nginx: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\\n/usr/sbin/nginx: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\\n/usr/sbin/nginx: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\\n/usr/sbin/nginx: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\\n/bin/bash: /azureml-envs/azureml_92b60d2fdd9f52c2d90c3626147e6f6b/lib/libtinfo.so.5: no version information available (required by /bin/bash)\\n2019-10-01T16:26:28,267812761+00:00 - iot-server/finish 1 0\\n2019-10-01T16:26:28,269766978+00:00 - Exit code 1 is normal. Not restarting iot-server.\\nStarting gunicorn 19.9.0\\nListening at: http://127.0.0.1:31311 (11)\\nUsing worker: sync\\nworker timeout is set to 300\\nBooting worker with pid: 39\\nInitializing logger\\nStarting up app insights client\\nStarting up request id generator\\nStarting up app insight hooks\\nInvoking user's init function\\nUsers's init has completed successfully\\nScoring timeout setting is not found. Use default timeout: 3600000 ms\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aci_service.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX - Predict\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt  \n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "aci_service = AciWebservice(ws,'onnx-demo-mnist-2')\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    \"\"\"Convert the input image into grayscale\"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def resize_img(img_to_resize):\n",
    "    \"\"\"Resize image to MNIST model input dimensions\"\"\"\n",
    "    r_img = cv2.resize(img_to_resize, dsize=(28, 28), interpolation=cv2.INTER_AREA)\n",
    "    r_img.resize((1, 1, 28, 28))\n",
    "    return r_img\n",
    "\n",
    "def preprocess(img_to_preprocess):\n",
    "    \"\"\"Resize input images and convert them to grayscale.\"\"\"\n",
    "    if img_to_preprocess.shape == (28, 28):\n",
    "        img_to_preprocess.resize((1, 1, 28, 28))\n",
    "        return img_to_preprocess\n",
    "    \n",
    "    grayscale = rgb2gray(img_to_preprocess)\n",
    "    processed_img = resize_img(grayscale)\n",
    "    return processed_img\n",
    "\n",
    "your_test_image = \"mnist/3.png\"\n",
    "img = cv2.imread(your_test_image)\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img, cmap = plt.cm.Greys)\n",
    "print(\"Old Dimensions: \", img.shape)\n",
    "img = preprocess(img)\n",
    "print(\"New Dimensions: \", img.shape)\n",
    "\n",
    "try:\n",
    "    input_data = json.dumps({'data': img.tolist()})\n",
    "    r = aci_service.run(input_data)\n",
    "    time_ms = np.round(r['time_in_sec'] * 1000, 2)\n",
    "    print(r)\n",
    "except KeyError as e:\n",
    "    print(str(e))\n",
    "\n",
    "plt.figure(figsize = (16, 6))\n",
    "plt.subplot(1, 15,1)\n",
    "plt.axhline('')\n",
    "plt.axvline('')\n",
    "plt.text(x = -100, y = -20, s = \"Model prediction: \", fontsize = 14)\n",
    "plt.text(x = -100, y = -10, s = \"Inference time: \", fontsize = 14)\n",
    "plt.text(x = 0, y = -20, s = str(r[\"result\"]), fontsize = 14)\n",
    "plt.text(x = 0, y = -10, s = str(time_ms) + \" ms\", fontsize = 14)\n",
    "plt.text(x = -100, y = 14, s = \"Input image: \", fontsize = 14)\n",
    "plt.imshow(img.reshape(28, 28), cmap = plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy models in other frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEARN - Register & Deploy\n",
    "from azureml.core import Environment, Model\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "sklearn_model = Model.register(ws,\"sklearn/model.pkl\",\"sklearn-model\")\n",
    "sklearn_env = Environment.from_conda_specification(\"sklearn_mnist\",\"sklearn/env.yml\")\n",
    "inference_config = InferenceConfig(environment=sklearn_env, \n",
    "                                   entry_script=\"sklearn/score.py\")\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1)\n",
    "\n",
    "sklearn_service = Model.deploy(ws, \"sklearn-svc\", [sklearn_model], inference_config, aciconfig)\n",
    "sklearn_service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5215.1981315798685, 3726.995485938578]\n"
     ]
    }
   ],
   "source": [
    "# SKLEARN - Predict\n",
    "import json\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "sklearn_service = AciWebservice(ws,\"sklearn-svc\")\n",
    "\n",
    "test_sample = json.dumps({'data': [\n",
    "    [1,2,3,4,5,6,7,8,9,10], \n",
    "    [10,9,8,7,6,5,4,3,2,1]\n",
    "]})\n",
    "\n",
    "test_sample_encoded = bytes(test_sample, encoding='utf8')\n",
    "prediction = sklearn_service.run(input_data=test_sample_encoded)\n",
    "\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
