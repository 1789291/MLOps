{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training using torch.distributed.launch module on Azure Machine Learning\n",
    "\n",
    "\n",
    "This example show how to train language model using the huggingface library  distributed on Azure Machine Learning using pytorch estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import wget\n",
    "import os\n",
    "\n",
    "from azureml.core import (Workspace, Experiment, \n",
    "                          VERSION, Datastore)\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.environment import Environment,CondaDependencies\n",
    "from azureml.train.dnn import PyTorch,Nccl\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "SUBSCRIPTION_ID = \"\"\n",
    "RESOURCE_GROUP = \"\"\n",
    "WORKSPACE_NAME = \"\"\n",
    "\n",
    "EXP_NAME = 'Azureml-LM_huggingface_example'\n",
    "CLUSTER_NAME = \"hf-cluster\"\n",
    "\n",
    "RUN_DIR = os.getcwd()\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "print(\"SDK version:\", VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace(subscription_id = SUBSCRIPTION_ID, \n",
    "               resource_group =RESOURCE_GROUP , \n",
    "               workspace_name = WORKSPACE_NAME\n",
    "              )\n",
    "\n",
    "\n",
    "exp = Experiment(workspace=ws, name=EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "wget.download(\"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\",\n",
    "              out=DATA_DIR\n",
    "             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "ds_reference = datastore.upload(src_dir='data',\n",
    "                 target_path='wikitext',\n",
    "                 overwrite=True,\n",
    "                 show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "\n",
    "\n",
    "found = False\n",
    "cts = ws.compute_targets\n",
    "if CLUSTER_NAME in cts and cts[CLUSTER_NAME].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[CLUSTER_NAME]\n",
    "\n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size =  'Standard_NC12',max_nodes = 8)\n",
    "\n",
    "    # Create the cluster.\\n\",\n",
    "    compute_target = ComputeTarget.create(ws, CLUSTER_NAME, provisioning_config)\n",
    "\n",
    "print('Checking cluster status...')\n",
    "compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $RUN_DIR/train.py\n",
    "\n",
    "import os\n",
    "import shutil \n",
    "import argparse\n",
    "import subprocess\n",
    "from git.repo.base import Repo\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "WORK_DIR = 'examples'\n",
    "SRC_DIR = '/transformers'\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(),'outputs')\n",
    "DATA_DIR = os.path.join(os.getcwd(),'wikitext-2-raw')\n",
    "\n",
    "REPO_URL=\"https://github.com/datashinobi/transformers.git\"\n",
    "BRANCH='yassine/aml_distributed'\n",
    "\n",
    "LOCAL_RANK = '0'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset-path', dest='ds_path')\n",
    "parser.add_argument('--rank', type=str,help='rank within nodes')\n",
    "parser.add_argument('--node_count', type=str,help='number of nodes')\n",
    "parser.add_argument('--process_per_node', type=str,help='number of process per node')\n",
    "parser.add_argument('--batch_size', type=str,help='training & eval batch size')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "#============Clone forked repo==========\n",
    "if os.path.exists(SRC_DIR):\n",
    "    print(\"huggingface repo exists, skip cloning\")\n",
    "else:\n",
    "    print('clone huggingface repo..........')\n",
    "    Repo.clone_from(REPO_URL,SRC_DIR, branch=BRANCH)\n",
    "\n",
    "#===============Unzip dataset=============\n",
    "data_file = os.path.join(args.ds_path,\"wikitext-2-raw-v1.zip\")\n",
    "with ZipFile(data_file,\"r\") as zip_file:\n",
    "    zip_file.extractall(os.getcwd())\n",
    "print(os.listdir(DATA_DIR))\n",
    "\n",
    "#===========start training=================\n",
    "master_node_params = os.environ['AZ_BATCH_MASTER_NODE'].split(':')\n",
    "print(\"MASTER node\", master_node_params)\n",
    "master_ip = master_node_params[0]\n",
    "master_port = master_node_params[1]\n",
    "\n",
    "process = subprocess.Popen(['python', '-m', 'torch.distributed.launch',\\\n",
    "                            '--nnodes',args.node_count,\\\n",
    "                            '--nproc_per_node',args.process_per_node,\\\n",
    "                            '--node_rank', args.rank,\\\n",
    "                            '--master_addr',master_ip,\\\n",
    "                            '--master_port',master_port,\\\n",
    "                            os.path.join(SRC_DIR, WORK_DIR, 'run_language_modeling.py'),\\\n",
    "                            '--output_dir', OUTPUT_DIR,\\\n",
    "                            '--model_type', 'roberta', \\\n",
    "                            '--model_name_or_path', 'roberta-base', \\\n",
    "                            '--do_train', \\\n",
    "                            '--train_data_file', os.path.join(DATA_DIR, 'wiki.train.raw'),\\\n",
    "                            '--do_eval', \\\n",
    "                            '--eval_data_file', os.path.join(DATA_DIR, 'wiki.test.raw'),\\\n",
    "                            '--mlm',\\\n",
    "                            '--local_rank', LOCAL_RANK,\\\n",
    "                            '--per_gpu_train_batch_size', args.batch_size,\\\n",
    "                            '--per_gpu_eval_batch_size', args.batch_size\n",
    "                             ],\n",
    "                           stdout=subprocess.PIPE,\n",
    "                           stderr=subprocess.STDOUT\n",
    "                        )\n",
    "\n",
    "lines_iterator = iter(process.stdout.readline, b\"\")\n",
    "while process.poll() is None:\n",
    "    for line in lines_iterator:\n",
    "        print(line, end = \"\\r\\n\",flush =True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count = 8\n",
    "process_per_node = 1\n",
    "\n",
    "script_params = {\n",
    "    '--dataset-path':ds_reference.as_mount(),\n",
    "    '--rank':'$AZ_BATCHAI_TASK_INDEX',\n",
    "    '--node_count':node_count,\n",
    "    '--process_per_node':process_per_node,\n",
    "    '--batch_size':'4'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "from azureml.train.estimator import Estimator\n",
    "est = PyTorch(source_directory=RUN_DIR,\n",
    "                pip_packages=['gitpython','scikit-learn','seqeval','tensorboardX',\\\n",
    "                              'tqdm','transformers'],\n",
    "                script_params=script_params,\n",
    "                use_gpu=True,\n",
    "                compute_target=compute_target,\n",
    "                entry_script=os.path.join(RUN_DIR,'train.py'),\n",
    "                framework_version='1.4',\n",
    "                node_count=node_count,\n",
    "                distributed_training=Nccl()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(est)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amlsamples_env]",
   "language": "python",
   "name": "conda-env-amlsamples_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
